# Metaloader - Metabolomics Data Loader

Open-source tool for loading metabolomics data (Metabolomics Workbench / MetaboAnalyst exports) into PostgreSQL.

## Phase 1: Foundation

This is the initial phase focusing on the foundation:
- Database schema and migrations
- File ingestion with deduplication
- Import tracking
- CLI interface

**Note:** This phase does NOT parse metabolite data yet - that comes in later phases.

## Features

- **File Ingestion**: Process metabolomics data files with automatic type detection
- **Deduplication**: Global deduplication by SHA256 hash and file size
- **Import Tracking**: Track batches of file imports with status and notes
- **Database Migrations**: Proper Alembic migrations for schema management
- **Type Detection**: Heuristic detection of file formats (mwtab, HTML tables, Excel, etc.)
- **CLI Interface**: Simple command-line interface using Typer

## Supported File Types

Currently detects (but doesn't parse yet):
- `.txt` - mwtab format, results text files
- `.htm`, `.html` - metabolite tables
- `.csv`, `.tsv` - CSV/TSV files
- `.xlsx`, `.xlsm` - Excel files (including NMR binned data)
- `.zip` - Archive files
- `.pdf` - PDF documents

## Installation

### Prerequisites

1. **Python 3.10+** - Check version:
   ```bash
   python3 --version
   ```

2. **PostgreSQL 15+** - Install via Homebrew on macOS:
   ```bash
   brew install postgresql@15
   brew services start postgresql@15
   ```

3. **uv** - Modern Python package manager (recommended):
   ```bash
   curl -LsSf https://astral.sh/uv/install.sh | sh
   ```

   Or use pip:
   ```bash
   pip install uv
   ```

### Database Setup

1. Create PostgreSQL database:
   ```bash
   createdb metaloader
   ```

2. Create a `.env` file in the project root:
   ```bash
   cp .env.example .env
   ```

3. Edit `.env` and configure your database connection:
   ```bash
   DATABASE_URL=postgresql://postgres:postgres@localhost:5432/metaloader
   LOG_LEVEL=INFO
   ```

   **Format:** `postgresql://[user]:[password]@[host]:[port]/[database]`

### Project Setup

1. Clone the repository:
   ```bash
   git clone <repository-url>
   cd metabo-workbench-parser
   ```

2. Install dependencies with uv:
   ```bash
   uv pip install -e .
   ```

   Or with pip:
   ```bash
   pip install -e .
   ```

3. Install development dependencies (optional):
   ```bash
   uv pip install -e ".[dev]"
   ```

## Usage

### 1. Test Database Connection

```bash
metaloader db ping
```

Expected output:
```
Testing database connection...
âœ“ Database connection successful!
Connected to: localhost:5432/metaloader
```

### 2. Initialize Database Schema

```bash
metaloader db init
```

This runs Alembic migrations to create all necessary tables:
- `imports` - Import batch tracking
- `files` - File registry with deduplication
- `studies`, `analyses`, `samples`, `features`, `measurements` - Placeholder tables

### 3. Ingest Files

Ingest a single file:

```bash
metaloader ingest-file /path/to/data/study_ST000001.txt
```

This will:
- Create a new import record
- Calculate SHA256 hash (streaming, memory-efficient)
- Detect file type heuristically
- Check for duplicates
- Store file metadata in database

Example output:
```
Ingesting file: /path/to/data/study_ST000001.txt
Created import: 550e8400-e29b-41d4-a716-446655440000
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Property           â”‚ Value                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Import ID          â”‚ 550e8400-e29b-41d4-a716-446655440000          â”‚
â”‚ File ID            â”‚ 660e9511-f3ac-52e5-b827-557766551111          â”‚
â”‚ Filename           â”‚ study_ST000001.txt                            â”‚
â”‚ Detected Type      â”‚ mwtab                                         â”‚
â”‚ SHA256             â”‚ a1b2c3d4...                                   â”‚
â”‚ Size (bytes)       â”‚ 45678                                         â”‚
â”‚ Status             â”‚ New                                           â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
âœ“ File ingested successfully!
```

Ingest to an existing import:

```bash
metaloader ingest-file /path/to/data/file2.html --import-id 550e8400-e29b-41d4-a716-446655440000
```

**Deduplication:** If the same file (same SHA256 + size) is ingested again, it will detect the duplicate and return the existing file ID without creating a new record.

### 4. Finalize Import

Mark an import as completed:

```bash
metaloader import finalize 550e8400-e29b-41d4-a716-446655440000 --status success --notes "Batch import completed"
```

Or mark as failed:

```bash
metaloader import finalize 550e8400-e29b-41d4-a716-446655440000 --status failed --notes "Error during processing"
```

### 5. Parse mwTab Files (Phase 2)

After ingesting files, parse mwTab files to extract samples, metabolites, and measurements.

**Find file_id from database:**

```sql
-- Connect to database
psql -d metaloader

-- List ingested mwTab files
SELECT id, filename, detected_type, created_at
FROM files
WHERE detected_type = 'mwtab'
ORDER BY created_at DESC
LIMIT 20;
```

**Parse using file_id (from database):**

```bash
metaloader parse mwtab <file-uuid>
```

**Parse using file path (direct):**

```bash
metaloader parse mwtab "/path/to/study_ST000315_AN000501.txt"
```

**Example output:**
```
Parsing mwTab: 660e9511-f3ac-52e5-b827-557766551111
Using file_id: 660e9511-f3ac-52e5-b827-557766551111
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Property                      â”‚ Value       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Study ID                      â”‚ ST000315    â”‚
â”‚ Analysis ID                   â”‚ AN000501    â”‚
â”‚                               â”‚             â”‚
â”‚ Samples                       â”‚             â”‚
â”‚   Processed                   â”‚ 233         â”‚
â”‚   Created                     â”‚ 233         â”‚
â”‚                               â”‚             â”‚
â”‚ Features (Metabolites)        â”‚             â”‚
â”‚   Processed                   â”‚ 156         â”‚
â”‚   Created                     â”‚ 156         â”‚
â”‚                               â”‚             â”‚
â”‚ Measurements                  â”‚             â”‚
â”‚   Processed                   â”‚ 36348       â”‚
â”‚   Inserted/Updated            â”‚ 36348       â”‚
â”‚                               â”‚             â”‚
â”‚ Warnings                      â”‚ 0           â”‚
â”‚ Mode                          â”‚ Production  â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
âœ“ File parsed and data stored successfully!
```

**Dry run mode** (preview without writing):

```bash
metaloader parse mwtab <file-uuid-or-path> --dry-run
```

**What gets parsed:**

1. **Metadata**: `STUDY_ID`, `ANALYSIS_ID` from file header
2. **Samples**: From `SUBJECT_SAMPLE_FACTORS` section
   - `sample_uid` = `{study_id}:{sample_label}`
   - Raw factors string saved to `factors_raw`
3. **Metabolites/Features**: From `MS_METABOLITE_DATA` section
   - `feature_uid` = `{analysis_id}:met:{normalized_name}`
   - `feature_type` = 'metabolite'
4. **Measurements**: Values from MS data table
   - Linked by `(sample_uid, feature_uid)`
   - Units detected from `MS_METABOLITE_DATA:UNITS`
   - NA/empty values stored as NULL

**Idempotency**: Running the same file twice will update existing records (upsert), not create duplicates.

### 6. Tag Files with Categories

After parsing files, tag them with inferred category values for exposure, device, sample type, and platform.

**Tag all files in database:**

```bash
metaloader files tag --all
```

**Tag files from specific import:**

```bash
metaloader files tag --import-id 550e8400-e29b-41d4-a716-446655440000
```

**Tag single file:**

```bash
metaloader files tag --file-id 660e9511-f3ac-52e5-b827-557766551111
```

**Dry run (preview without writing):**

```bash
metaloader files tag --all --dry-run
```

**Overwrite existing values:**

```bash
metaloader files tag --all --overwrite
```

**Example output:**
```
Tagging files with category values
Tagging all files in database
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Metric                  â”‚    Value â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Files processed         â”‚       42 â”‚
â”‚ Files updated           â”‚       38 â”‚
â”‚ Files skipped           â”‚        4 â”‚
â”‚                         â”‚          â”‚
â”‚ Tags set:               â”‚          â”‚
â”‚   Device                â”‚       35 â”‚
â”‚   Exposure              â”‚       22 â”‚
â”‚   Sample type           â”‚       40 â”‚
â”‚   Platform              â”‚       28 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
âœ“ Tagging complete: 38 files updated
```

**Inferred categories:**

| Category | Values | Detection Logic |
|----------|--------|-----------------|
| **device** | `NMR`, `LCMS`, `GCMS` | Based on `detected_type` and path patterns |
| **exposure** | `OB`, `CON` | Keywords: obese/OB, control/CON/lean |
| **sample_type** | `Serum`, `Urine`, `Feces`, `CSF` | Keywords in path/filename |
| **platform** | `ESI_pos`, `HILIC`, `QQQ`, etc. | Ionization, chromatography, mass analyzer patterns |

### 7. Export Data to Parquet

Export long-format measurement data to Parquet for analysis in R or Python.

**Basic export:**

```bash
metaloader export parquet --out exports/metaloader_long.parquet
```

**Filter by import:**

```bash
metaloader export parquet --out exports/data.parquet --import-id 550e8400-e29b-41d4-a716-446655440000
```

**Filter by feature type:**

```bash
metaloader export parquet --out exports/nmr_data.parquet --feature-type nmr_bin
```

**Preview data without exporting:**

```bash
metaloader export parquet --out exports/test.parquet --preview
```

**Count rows before export:**

```bash
metaloader export parquet --out exports/test.parquet --count
```

**Example output:**
```
Exporting measurement data to Parquet
Output: exports/metaloader_long.parquet
Chunk size: 200,000
Processing chunk 1: 200,000 rows
Processing chunk 2: 156,348 rows
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Property          â”‚ Value                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Output file       â”‚ exports/metaloader_long.parquet â”‚
â”‚ Total rows        â”‚ 356,348                        â”‚
â”‚ Chunks written    â”‚ 2                              â”‚
â”‚ File size         â”‚ 12.45 MB                       â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
âœ“ Export complete: 356,348 rows written to exports/metaloader_long.parquet
```

**Exported columns:**

- `file_id`, `path_rel`, `detected_type` - File metadata
- `device`, `exposure`, `sample_type`, `platform` - Category tags
- `sample_uid`, `sample_label` - Sample identifiers
- `feature_uid`, `feature_type`, `feature_name`, `refmet_name` - Feature info
- `value`, `unit`, `col_index`, `replicate_ix` - Measurement data
- `study_id`, `analysis_id` - Study/analysis references
- `created_at` - Timestamp

### Reading Parquet in R

```r
library(arrow)

# Read entire file
df <- read_parquet("metaloader_long.parquet")

# Or use lazy evaluation for large files
dataset <- open_dataset("metaloader_long.parquet")

# Filter and collect
nmr_data <- dataset |>
  filter(device == "NMR") |>
  collect()

# Filter by exposure
ob_samples <- dataset |>
  filter(exposure == "OB") |>
  select(sample_uid, feature_name, value) |>
  collect()

# Aggregate by sample type
sample_counts <- dataset |>
  group_by(sample_type) |>
  summarise(n = n(), mean_value = mean(value, na.rm = TRUE)) |>
  collect()
```

### Reading Parquet in Python

```python
import pandas as pd
import pyarrow.parquet as pq

# Read entire file
df = pd.read_parquet("metaloader_long.parquet")

# Or use PyArrow for lazy filtering
table = pq.read_table(
    "metaloader_long.parquet",
    filters=[("device", "==", "NMR")]
)
df = table.to_pandas()

# With polars (recommended for large files)
import polars as pl
df = pl.scan_parquet("metaloader_long.parquet") \
    .filter(pl.col("exposure") == "OB") \
    .collect()
```

**Check parsed data:**

```sql
-- Overview of studies and analyses
SELECT s.study_id, a.analysis_id,
       COUNT(DISTINCT sam.id) as samples,
       COUNT(DISTINCT f.id) as features,
       COUNT(DISTINCT m.id) as measurements
FROM studies s
LEFT JOIN analyses a ON a.study_pk = s.id
LEFT JOIN samples sam ON sam.study_pk = s.id
LEFT JOIN features f ON f.feature_uid LIKE a.analysis_id || ':%'
LEFT JOIN measurements m ON m.sample_uid = sam.sample_uid
GROUP BY s.study_id, a.analysis_id;

-- View samples for a study
SELECT sample_uid, sample_label, factors_raw
FROM samples
WHERE study_pk = (SELECT id FROM studies WHERE study_id = 'ST000315')
LIMIT 10;

-- View metabolites for an analysis
SELECT feature_uid, name_raw, feature_type
FROM features
WHERE feature_uid LIKE 'AN000501:%'
LIMIT 10;

-- View measurements for a sample
SELECT f.name_raw, m.value, m.unit
FROM measurements m
JOIN features f ON f.feature_uid = m.feature_uid
WHERE m.sample_uid = 'ST000315:Sample1'
ORDER BY f.name_raw;

-- Count measurements per study
SELECT s.study_id, COUNT(*) as measurement_count
FROM measurements m
JOIN samples sam ON sam.sample_uid = m.sample_uid
JOIN studies s ON s.id = sam.study_pk
GROUP BY s.study_id;
```

## Database Schema

### Core Tables (Phase 1)

**imports**
- Tracks import batches
- Fields: `id`, `created_at`, `root_path`, `status`, `notes`
- Status: `running`, `success`, `failed`

**files**
- File registry with global deduplication
- Fields: `id`, `import_id`, `path_abs`, `path_rel`, `filename`, `ext`, `size_bytes`, `sha256`, `detected_type`, `created_at`
- Unique constraint: `(sha256, size_bytes)`
- Index on: `sha256`

### Data Tables (Phase 2)

**studies**
- Study metadata
- Fields: `id`, `study_id`, `created_at`
- Upsert by: `study_id`

**analyses**
- Analysis metadata
- Fields: `id`, `study_pk`, `analysis_id`, `file_id`, `created_at`
- Foreign keys: `study_pk` â†’ `studies.id`, `file_id` â†’ `files.id`
- Upsert by: `(study_pk, analysis_id)`

**samples**
- Sample information
- Fields: `id`, `study_pk`, `sample_label`, `sample_uid`, `factors_raw`, `created_at`
- Unique constraint: `sample_uid`
- `sample_uid` format: `{study_id}:{sample_label}`
- `factors_raw`: Raw factors string from mwTab (e.g., "Group:Exercise | Visit:1")

**sample_factors**
- Sample metadata key-value pairs (parsed from factors_raw)
- Fields: `id`, `sample_uid`, `factor_key`, `factor_value`, `created_at`
- Unique constraint: `(sample_uid, factor_key)`
- Foreign key: `sample_uid` â†’ `samples.sample_uid` (ON DELETE CASCADE)

**features**
- Metabolite/feature definitions
- Fields: `id`, `feature_uid`, `feature_type`, `name_raw`, `created_at`
- Unique constraint: `feature_uid`
- `feature_uid` format: `{analysis_id}:met:{normalized_name}`
- `feature_type`: 'metabolite' for MS data

**measurements**
- Feature values per sample
- Fields: `id`, `sample_uid`, `feature_uid`, `value`, `unit`, `created_at`
- Unique constraint: `(sample_uid, feature_uid)` - enables upsert
- Foreign keys: `sample_uid` â†’ `samples.sample_uid`, `feature_uid` â†’ `features.feature_uid`
- `value`: Float, NULL for missing/NA values
- `unit`: Detected from `MS_METABOLITE_DATA:UNITS`

## File Type Detection

Heuristic rules:

| Pattern | Detected Type |
|---------|---------------|
| `.txt` containing `#METABOLOMICS WORKBENCH` | `mwtab` |
| `.htm/.html` containing `Metabolite_name` | `metabo_table_html` |
| Filename contains `_res.txt` | `results_txt` |
| `.xlsx/.xlsm` with `normalized binned data` in name | `nmr_binned_xlsx` |
| Others | `unknown` |

## Development

### Running Tests

```bash
pytest
```

Run with coverage:
```bash
pytest --cov=metaloader --cov-report=html
```

### Code Quality

Format and lint:
```bash
ruff check src/ tests/
ruff format src/ tests/
```

## Architecture

```
src/metaloader/
â”œâ”€â”€ cli.py              # CLI commands (Typer)
â”œâ”€â”€ config.py           # Configuration (env vars)
â”œâ”€â”€ database.py         # SQLAlchemy setup
â”œâ”€â”€ models.py           # Database models
â”œâ”€â”€ parsers/
â”‚   â””â”€â”€ mwtab.py           # mwTab format parser
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ file_handler.py    # File processing logic
â”‚   â”œâ”€â”€ import_service.py  # Import management
â”‚   â”œâ”€â”€ parse_service.py   # Parsing and data extraction
â”‚   â”œâ”€â”€ tagger_service.py  # File category tagging
â”‚   â””â”€â”€ export_service.py  # Parquet export
â””â”€â”€ utils/
    â”œâ”€â”€ hashing.py         # SHA256 streaming
    â”œâ”€â”€ type_detector.py   # File type detection
    â””â”€â”€ tagger.py          # Category inference heuristics

alembic/
â”œâ”€â”€ env.py              # Alembic environment
â””â”€â”€ versions/
    â”œâ”€â”€ 001_initial_schema.py
    â”œâ”€â”€ 002_add_sample_factors.py
    â”œâ”€â”€ 003_add_factors_raw_and_measurements_constraint.py
    â”œâ”€â”€ 004_add_ms_measurement_columns.py
    â”œâ”€â”€ 005_add_category_columns.py
    â”œâ”€â”€ 006_add_file_parse_status.py
    â””â”€â”€ 007_add_file_category_columns.py

tests/
â”œâ”€â”€ test_hashing.py
â”œâ”€â”€ test_type_detector.py
â”œâ”€â”€ test_mwtab_parser.py
â””â”€â”€ test_tagger.py
```

## Troubleshooting

### Database Connection Issues

1. Check PostgreSQL is running:
   ```bash
   brew services list | grep postgresql
   ```

2. Test connection manually:
   ```bash
   psql -h localhost -U postgres -d metaloader
   ```

3. Verify DATABASE_URL in `.env`:
   ```bash
   cat .env | grep DATABASE_URL
   ```

### Migration Issues

Reset database (âš ï¸ destroys all data):
```bash
dropdb metaloader
createdb metaloader
metaloader db init
```

## Roadmap

- âœ… **Phase 1**: Foundation
  - Database schema
  - File ingestion with deduplication
  - Import tracking

- âœ… **Phase 2**: mwTab Parsing
  - Parse `SUBJECT_SAMPLE_FACTORS` section
  - Parse `MS_METABOLITE_DATA` section
  - Extract study/analysis/sample metadata
  - Create features and measurements
  - Batch upserts for performance
  - Idempotent: re-running updates existing data

- âœ… **Phase 3**: Export & Categories
  - Category tagging: device, exposure, sample_type, platform
  - Heuristic inference from file paths and metadata
  - Parquet export with chunked streaming
  - zstd compression for efficient storage

- ğŸ“‹ **Phase 4**: Additional Formats (planned)
  - NMR binned data parser (Excel)
  - HTML table parser
  - CSV/TSV parsers

- ğŸ“‹ **Phase 5**: Analysis (planned)
  - Data validation
  - Quality checks
  - Data aggregation/normalization

## Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Run tests: `pytest`
5. Submit a pull request

## License

MIT License - See LICENSE file for details

## Support

For issues and questions:
- Open an issue on GitHub
- Check existing issues for solutions

## Authors

- Your Name <your.email@example.com>